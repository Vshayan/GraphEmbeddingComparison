{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing node2vec, HOPE and SDNE on datasets(Karate, Dolphin, LFR, American_football, book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Miniconda3\\envs\\graph_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from networkx import to_numpy_array\n",
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sknetwork.data import karate_club\n",
    "from sknetwork.clustering import get_modularity\n",
    "from sknetwork.visualization import svg_graph\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating graphs from data\n",
    "\n",
    "graph_karate = nx.karate_club_graph()\n",
    "\n",
    "graph_dolphin = nx.Graph()\n",
    "\n",
    "with open('soc-dolphins.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        node1, node2 = line.strip().split()\n",
    "        graph_dolphin.add_edge(node1, node2)\n",
    "\n",
    "graph_football = nx.Graph()\n",
    "\n",
    "with open('American_football.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        node1, node2 = line.strip().split()\n",
    "        graph_football.add_edge(node1, node2)\n",
    "\n",
    "\n",
    "graph_book = nx.Graph()\n",
    "\n",
    "with open('book.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        node1, node2 = line.strip().split()\n",
    "        graph_book.add_edge(node1, node2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating adjancency and position\n",
    "def get_adjacency_and_position(graph):\n",
    "  adjacency = csr_matrix(nx.to_numpy_array(graph))\n",
    "  position_dict = nx.spring_layout(graph)\n",
    "  position = np.array(list(position_dict.values()))\n",
    "  return adjacency, position\n",
    "\n",
    "\n",
    "def get_node_array_without_embedding(graph):\n",
    "  '''\n",
    "  Convert graph to array without embedding\n",
    "  '''\n",
    "  node_array = to_numpy_array(graph)\n",
    "  return node_array\n",
    "\n",
    "\n",
    "# Node2vec\n",
    "def get_embedded_node_array_using_node2vec(graph):\n",
    "\n",
    "    model = Node2Vec(graph, dimensions=64, walk_length=80, num_walks=100, p=0.5, q=1.0)\n",
    "    model = model.fit(window=10, min_count=1)\n",
    "\n",
    "    embeddings = {node: model.wv[str(node)] for node in graph.nodes()}\n",
    "    embedded_node_array = np.array(list(embeddings.values()))\n",
    "\n",
    "    norm = np.linalg.norm(embedded_node_array, axis=1, keepdims=True)\n",
    "\n",
    "    embedded_node_array = embedded_node_array / norm\n",
    "    return embedded_node_array\n",
    "\n",
    "# HOPE\n",
    "def get_hope_embedding(graph, d=32):\n",
    "\n",
    "    adjacency_matrix = nx.to_numpy_array(graph)\n",
    "    identity_matrix = np.eye(adjacency_matrix.shape[0])\n",
    "\n",
    "    katz_matrix = np.linalg.inv(identity_matrix - 0.5 * adjacency_matrix) - identity_matrix\n",
    "    katz_matrix = np.asarray(katz_matrix)  \n",
    "\n",
    "    svd = TruncatedSVD(n_components=d)\n",
    "    U = svd.fit_transform(katz_matrix)\n",
    "    S = svd.singular_values_\n",
    "    V = svd.components_.T\n",
    "\n",
    "    hope_embedding = np.concatenate((U * np.sqrt(S), V * np.sqrt(S)), axis=1)\n",
    "\n",
    "    norm = np.linalg.norm(hope_embedding, axis=1, keepdims=True)\n",
    "    \n",
    "    hope_embedding = hope_embedding / norm\n",
    "    return hope_embedding\n",
    "\n",
    "# SDNE\n",
    "def train_sdne(graph, hidden_layers=[256, 64], epochs=100, learning_rate=0.01):    \n",
    "    class SDNE(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_layers, alpha=1e-5, beta=5):\n",
    "            super(SDNE, self).__init__()\n",
    "            self.alpha = alpha\n",
    "            self.beta = beta\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_layers[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_layers[0], hidden_layers[1])\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(hidden_layers[1], hidden_layers[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_layers[0], input_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            reconstructed = self.decoder(encoded)\n",
    "            return encoded, reconstructed\n",
    "\n",
    "        def loss(self, x, reconstructed, L):\n",
    "            bce_loss = nn.BCEWithLogitsLoss()(reconstructed, x)\n",
    "            L_loss = self.beta * torch.mean(torch.sum(L * (x - reconstructed) ** 2, dim=1))\n",
    "            return bce_loss + self.alpha * L_loss\n",
    "\n",
    "    def get_adjacency_matrix(graph):\n",
    "        adjacency_matrix = nx.to_numpy_array(graph)\n",
    "        return adjacency_matrix\n",
    "\n",
    "    adjacency_matrix = get_adjacency_matrix(graph)\n",
    "    L = torch.FloatTensor(adjacency_matrix)\n",
    "    D = np.diag(np.sum(adjacency_matrix, axis=1))\n",
    "    L = D - adjacency_matrix\n",
    "    L = torch.FloatTensor(L)\n",
    "\n",
    "    model = SDNE(adjacency_matrix.shape[1], hidden_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    adjacency_matrix = torch.FloatTensor(adjacency_matrix)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        _, reconstructed = model(adjacency_matrix)\n",
    "        loss = model.loss(adjacency_matrix, reconstructed, L)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding, _ = model(adjacency_matrix)\n",
    "\n",
    "    embedding = embedding.numpy()\n",
    "    norm = np.linalg.norm(embedding, axis=1, keepdims=True)\n",
    "    embedding = embedding / norm\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data array\n",
    "\n",
    "def prepare_data_arrays(graph):\n",
    "    node2vec = get_embedded_node_array_using_node2vec(graph)\n",
    "    hope = get_hope_embedding(graph)\n",
    "    sdne = train_sdne(graph)\n",
    "    \n",
    "    return {\n",
    "        'node_array': get_node_array_without_embedding(graph),\n",
    "        'embedded_node_array_node2vec': node2vec,\n",
    "        'embedded_node_array_hope': hope,\n",
    "        'embedded_node_array_sdne': sdne,\n",
    "        'embedded_node_array_sdne+hope': np.add(sdne, hope),\n",
    "        'embedded_node_array_sdne+node2vec': np.add(sdne, node2vec),\n",
    "        'embedded_node_array_hope+node2vec': np.add(hope, node2vec),\n",
    "        'embedded_node_array_summation': np.add(np.add(sdne, hope), node2vec),\n",
    "        'embedded_node_array_sdne*hope': np.multiply(sdne, hope),\n",
    "        'embedded_node_array_sdne*node2vec': np.multiply(sdne, node2vec),\n",
    "        'embedded_node_array_hope*node2vec': np.multiply(hope, node2vec),\n",
    "        'embedded_node_array_multiply': np.multiply(np.multiply(sdne, hope), node2vec),\n",
    "        'embedded_node_array_sdneMINhope': np.minimum(sdne, hope),\n",
    "        'embedded_node_array_sdneMINnode2vec': np.minimum(sdne, node2vec),\n",
    "        'embedded_node_array_hopeMINnode2vec': np.minimum(hope, node2vec),\n",
    "        'embedded_node_array_minimum': np.minimum(np.minimum(sdne, hope), node2vec),\n",
    "        'embedded_node_array_sdneMAXhope': np.maximum(sdne, hope),\n",
    "        'embedded_node_array_sdneMAXnode2vec': np.maximum(sdne, node2vec),\n",
    "        'embedded_node_array_hopeMAXnode2vec': np.maximum(hope, node2vec),\n",
    "        'embedded_node_array_maximum': np.maximum(np.maximum(sdne, hope), node2vec),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating adjancency and position for each dataset\n",
    "\n",
    "graph = karate_club(metadata=True)\n",
    "adjacency_karate, position_karate = graph.adjacency, graph.position\n",
    "adjacency_dolphin, position_dolphin = get_adjacency_and_position(graph_dolphin)\n",
    "adjacency_football, position_football = get_adjacency_and_position(graph_football)\n",
    "adjacency_book, position_book = get_adjacency_and_position(graph_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 34/34 [00:00<00:00, 6164.36it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:01<00:00, 89.19it/s]\n",
      "Computing transition probabilities: 100%|██████████| 62/62 [00:00<00:00, 4383.72it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:02<00:00, 49.58it/s]\n",
      "Computing transition probabilities: 100%|██████████| 115/115 [00:00<00:00, 1836.58it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:04<00:00, 23.67it/s]\n",
      "Computing transition probabilities: 100%|██████████| 105/105 [00:00<00:00, 2096.19it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:03<00:00, 25.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating data array for each dataset\n",
    "\n",
    "karate_club_data_array = prepare_data_arrays(graph_karate)\n",
    "dolphin_data_array = prepare_data_arrays(graph_dolphin)\n",
    "football_data_array = prepare_data_arrays(graph_football)\n",
    "book_data_array = prepare_data_arrays(graph_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "\n",
    "def perform_kmeans_clustering(data_array, adjacency, range_k=(2, 11), random_state=0):\n",
    "    labels = []\n",
    "    modularity = []\n",
    "    work_log = []\n",
    "\n",
    "    for k in range(*range_k):\n",
    "        start_time = datetime.now()\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "        kmeans.fit(data_array)\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        labels.append(kmeans.labels_)\n",
    "        modularity.append(get_modularity(adjacency, kmeans.labels_))\n",
    "        work_log.append(end_time - start_time)\n",
    "\n",
    "    best_k = modularity.index(max(modularity)) + range_k[0]\n",
    "    best_labels = labels[best_k - range_k[0]]\n",
    "    best_modularity = modularity[best_k - range_k[0]]\n",
    "    best_work_log = work_log[best_k - range_k[0]]\n",
    "\n",
    "    # print(f\"Best value of k: {best_k}\")\n",
    "    return best_labels, best_modularity, best_work_log\n",
    "\n",
    "def cluster_all_data_arrays(data_arrays, adjacency):\n",
    "    labels_dict = {}\n",
    "    modularity_dict = {}\n",
    "    work_log_dict = {}\n",
    "\n",
    "    for array_type, data_array in data_arrays.items():\n",
    "        # print(f\"Clustering for {array_type}...\")\n",
    "        best_labels, best_modularity, best_work_log = perform_kmeans_clustering(data_array, adjacency)\n",
    "        \n",
    "        labels_dict[array_type] = best_labels\n",
    "        modularity_dict[array_type] = best_modularity\n",
    "        work_log_dict[array_type] = best_work_log\n",
    "    \n",
    "    return labels_dict, modularity_dict, work_log_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing K-means\n",
    "\n",
    "kmeans_labels_dict_karate, kmeans_modularity_karate, kmeans_work_log_karate = cluster_all_data_arrays(karate_club_data_array, adjacency_karate)\n",
    "kmeans_labels_dict_dolphin, kmeans_modularity_dolphin, kmeans_work_log_dolphin = cluster_all_data_arrays(dolphin_data_array, adjacency_dolphin)\n",
    "kmeans_labels_dict_football, kmeans_modularity_football, kmeans_work_log_football = cluster_all_data_arrays(football_data_array, adjacency_football)\n",
    "kmeans_labels_dict_book, kmeans_modularity_book, kmeans_work_log_book = cluster_all_data_arrays(book_data_array, adjacency_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_array': 0.11119329388560151,\n",
       " 'embedded_node_array_node2vec': 0.40327087442472065,\n",
       " 'embedded_node_array_hope': 0.21572978303747536,\n",
       " 'embedded_node_array_sdne': 0.13280736357659406,\n",
       " 'embedded_node_array_sdne+hope': 0.22098948060486523,\n",
       " 'embedded_node_array_sdne+node2vec': 0.40327087442472065,\n",
       " 'embedded_node_array_hope+node2vec': 0.23528928336620644,\n",
       " 'embedded_node_array_summation': 0.2693129520052597,\n",
       " 'embedded_node_array_sdne*hope': 0.20603221564760033,\n",
       " 'embedded_node_array_sdne*node2vec': 0.39907955292570674,\n",
       " 'embedded_node_array_hope*node2vec': 0.1068376068376069,\n",
       " 'embedded_node_array_multiply': 0.06730769230769218,\n",
       " 'embedded_node_array_sdneMINhope': 0.19156804733727806,\n",
       " 'embedded_node_array_sdneMINnode2vec': 0.3924227481919791,\n",
       " 'embedded_node_array_hopeMINnode2vec': 0.21745562130177512,\n",
       " 'embedded_node_array_minimum': 0.22238658777120318,\n",
       " 'embedded_node_array_sdneMAXhope': 0.21194937541091385,\n",
       " 'embedded_node_array_sdneMAXnode2vec': 0.39907955292570674,\n",
       " 'embedded_node_array_hopeMAXnode2vec': 0.2952827087442472,\n",
       " 'embedded_node_array_maximum': 0.23479618671926364}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_modularity_karate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_array': 0.24597523832126889,\n",
       " 'embedded_node_array_node2vec': 0.5231399074403703,\n",
       " 'embedded_node_array_hope': 0.19057790435504934,\n",
       " 'embedded_node_array_sdne': 0.10242870139630555,\n",
       " 'embedded_node_array_sdne+hope': 0.1307701435860923,\n",
       " 'embedded_node_array_sdne+node2vec': 0.3131996360903445,\n",
       " 'embedded_node_array_hope+node2vec': 0.34021597246944346,\n",
       " 'embedded_node_array_summation': 0.20155452711522492,\n",
       " 'embedded_node_array_sdne*hope': 0.06578062576638585,\n",
       " 'embedded_node_array_sdne*node2vec': 0.25380720699339426,\n",
       " 'embedded_node_array_hope*node2vec': 0.12246351014595946,\n",
       " 'embedded_node_array_multiply': 0.07102171591313644,\n",
       " 'embedded_node_array_sdneMINhope': 0.13361813219413798,\n",
       " 'embedded_node_array_sdneMINnode2vec': 0.28750840552193346,\n",
       " 'embedded_node_array_hopeMINnode2vec': 0.32925912740793484,\n",
       " 'embedded_node_array_minimum': 0.13747478343419958,\n",
       " 'embedded_node_array_sdneMAXhope': 0.1307701435860923,\n",
       " 'embedded_node_array_sdneMAXnode2vec': 0.2600965151694949,\n",
       " 'embedded_node_array_hopeMAXnode2vec': 0.3403148609627784,\n",
       " 'embedded_node_array_maximum': 0.24739923262529173}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_modularity_dolphin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_array': 0.5704156010740642,\n",
       " 'embedded_node_array_node2vec': 0.5638530586610391,\n",
       " 'embedded_node_array_hope': 0.058122144189649005,\n",
       " 'embedded_node_array_sdne': 0.530166139303668,\n",
       " 'embedded_node_array_sdne+hope': 0.22637577873640446,\n",
       " 'embedded_node_array_sdne+node2vec': 0.5853343410446312,\n",
       " 'embedded_node_array_hope+node2vec': 0.30940152061505877,\n",
       " 'embedded_node_array_summation': 0.29857971253615917,\n",
       " 'embedded_node_array_sdne*hope': 0.14256364947614086,\n",
       " 'embedded_node_array_sdne*node2vec': 0.5575074048151923,\n",
       " 'embedded_node_array_hope*node2vec': 0.062031460817683515,\n",
       " 'embedded_node_array_multiply': 0.06946821052295454,\n",
       " 'embedded_node_array_sdneMINhope': 0.22531262557581908,\n",
       " 'embedded_node_array_sdneMINnode2vec': 0.5885530738299327,\n",
       " 'embedded_node_array_hopeMINnode2vec': 0.26112052883553455,\n",
       " 'embedded_node_array_minimum': 0.254424925951848,\n",
       " 'embedded_node_array_sdneMAXhope': 0.20870135641843776,\n",
       " 'embedded_node_array_sdneMAXnode2vec': 0.540415787358723,\n",
       " 'embedded_node_array_hopeMAXnode2vec': 0.28293712360519363,\n",
       " 'embedded_node_array_maximum': 0.28106629338769296}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_modularity_football"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_array': 0.3640432741501741,\n",
       " 'embedded_node_array_node2vec': 0.5119060473773789,\n",
       " 'embedded_node_array_hope': 0.14496017605832962,\n",
       " 'embedded_node_array_sdne': 0.4316617047423654,\n",
       " 'embedded_node_array_sdne+hope': 0.27013949948838195,\n",
       " 'embedded_node_array_sdne+node2vec': 0.4478046698649222,\n",
       " 'embedded_node_array_hope+node2vec': 0.2721062725921813,\n",
       " 'embedded_node_array_summation': 0.4404646212226385,\n",
       " 'embedded_node_array_sdne*hope': 0.135481100981587,\n",
       " 'embedded_node_array_sdne*node2vec': 0.4556563366087175,\n",
       " 'embedded_node_array_hope*node2vec': 0.18724965420786588,\n",
       " 'embedded_node_array_multiply': 0.09695034476375591,\n",
       " 'embedded_node_array_sdneMINhope': 0.29376648618631107,\n",
       " 'embedded_node_array_sdneMINnode2vec': 0.446817426895172,\n",
       " 'embedded_node_array_hopeMINnode2vec': 0.33349530288305795,\n",
       " 'embedded_node_array_minimum': 0.4247998519135544,\n",
       " 'embedded_node_array_sdneMAXhope': 0.2874471028018162,\n",
       " 'embedded_node_array_sdneMAXnode2vec': 0.4598932543538956,\n",
       " 'embedded_node_array_hopeMAXnode2vec': 0.22876270689681774,\n",
       " 'embedded_node_array_maximum': 0.30765473233889173}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_modularity_book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
